[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to brms",
    "section": "",
    "text": "Overview\nWelcome to this mini-workshop on brms! The goals of this workshop and reader are to provide a resource for you to learn about:\n\nThe package’s capabilities and utility in a Bayesian workflow\nHow brms uses Stan\nA generalized linear mixed-model example that uses brms\n\nNo prerequisites required, however it would be helpful to have an general understanding of Bayesian statistics."
  },
  {
    "objectID": "01_intro.html#bayesian-stats-is-much-more-than-just-fitting-a-model",
    "href": "01_intro.html#bayesian-stats-is-much-more-than-just-fitting-a-model",
    "title": "Background",
    "section": "Bayesian stats is much more than just fitting a model",
    "text": "Bayesian stats is much more than just fitting a model\nBayesian statistics require several steps with key decision-making moments along the way that could greatly impact your analysis. Gelman et al. (2020) outlines a workflow that helps navigate through the critical decision points:\n\nPick an initial model\nFit the model\nValidate computation\nAddress computational issues\nEvaluate and use model OR modify the model\nCompare models\n\nThis workshop covers how aspects of the brms package can be applied in each step of the Bayesian workflow. This is by no means a comprehensive list of things you should consider in your workflow, but more like a starter pack. Check out the References section of this reader for resources with a more comprehensive look at Bayesian statistics."
  },
  {
    "objectID": "02_brms.html#brms---a-package-that-can-help-you-throughout-your-bayesian-workflow",
    "href": "02_brms.html#brms---a-package-that-can-help-you-throughout-your-bayesian-workflow",
    "title": "brms in application",
    "section": "brms - a package that can help you throughout your Bayesian workflow!",
    "text": "brms - a package that can help you throughout your Bayesian workflow!\nbrms (short for Bayesian Regression Models using ‘Stan’) is a package that allows for user friendly specification of Bayesian models. The package was developed by Paul-Christian Bürkner. If you are mostly familiar with linear regression of the frequentist sort, you are in luck: the formula syntax in brms is very similar to that in lme4! The brms README file links to several resources including extensive package documentation, vignettes, and forum discussions\nIf you are new to Bayesian stats or are in need of a little brush up, check out this Bayesian Primer by Van de Schoot et al. (2021)."
  },
  {
    "objectID": "02_brms.html#install-brms",
    "href": "02_brms.html#install-brms",
    "title": "brms in application",
    "section": "Install brms",
    "text": "Install brms\nYou will first need to download brms.\n\n## Latest release from CRAN\n install.packages(\"brms\")\n\n## Or the latest development version from GitHub\n devtools::install_github(\"paul-buerkner/brms\")"
  },
  {
    "objectID": "02_brms.html#brms-the-bayesian-workflow",
    "href": "02_brms.html#brms-the-bayesian-workflow",
    "title": "brms in application",
    "section": "brms & the Bayesian workflow",
    "text": "brms & the Bayesian workflow\n\n1. Pick an initial model\nbrm is the modelling function where you can specify anything that may be relevant to your model (e.g., the response distribution, priors, group-level factors, number of chains, iterations, etc.).\nSome of the core arguments you will need to specify are:\n\nformula: this is where you specify your response and predictor variables of interest (e.g., y ~ x). This is also where you will specify your random effects (e.g., y ~ x1 + (1|x2)\ndata: specify your dataframe\nfamily: brms can handle a wide range of response distributions linear, count data, survival, response times, ordinal, zero-inflated, self-defined mixture models and much much more! This is also where you specify your link argument\n\nOther arguments you might want to specify:\n\nprior: specify the prior distributions you would like for your population-level and group-level variables. You are also not restricted to just normal distributions and specify different types including uniform, Cauchy, Gamma, etc. If you do not specify your priors, the model assumes an uninformed flat prior distribution [double check this].\nwarmup: number of burn in iterations to help model find stable sampling space (default is iter/2)\niter: number of total iterations per chain (default is 2000)\nchains: number of markov chains you would like to run (default is 4)\ncores: number of cores to use when chains are running (default is 1), but you may want to specify more if your computer has capability. You can check how many cores you have by using parallel::detectCores()\ncontrol: adjust the sampling behavior of the model. I’ve mostly used this to help lower the number of divergent transitions (e.g., control = list(adapt_delta(x=.9)))\nfile: you can tell the model where to save its results (e.g., file = paste0(model,“/m_results”)\n\nAnother modelling function is brm_multiple for when you have multiple datasets that you would like to run your model over. Specifically, this is helpful if you missing data and your dataset would benefit from multiple imputation. brms interfaces nicely with the mice package which allows you to pass multiple imputed datasets into your model which then combines the posteriors draws into one model result. You can then run the rest of the brms post-processing methods as you typically would.\n\n\n2. Fit the model\n\nFigure 1. brms model fitting procedure (Burkner, 2017)\n\n\n3. Validate computation\nThis part of the workflow stresses the use of simulated data to make sure the model takes a reasonable length of time to run & passes convergence diagnostics.\nCheck out your model results: the summary function will spit out key model information like the family, formula, data and number of observations used in the model, number of samples, WAIC score, population-level effects and group-level effects (if you have any) with their respective estimates, error, 95% confidence interval, effective sample size, and Rhat score.\nEffective sample size and Rhat score are two default metrics of convergence. If your effective sample size is much lower than your number of iterations there could be efficiency issues in your chains. Secondly, if your Rhat score is > 1, then this indicates some convergence issues and therefore may not have reliable results.\n\n\n4. Address computational issues\nIn general, it is helpful to start with a simple version of the model to diagnose exactly where you are running into issues. Within your argument specification in the brm function, you can play around with the following:\n\nIf your model is taking a long time to run, try a smaller simulated dataset or a subset of your data, lower the number of iter from the default (which is 2000), or add in more informative priors than you had previously by adjusting the prior argument.\nIf your model is having convergence issues, you can try adjusting the control function to be greater than the default, which is .8, up to 1.\n\n\n\n5. Evaluate and use model OR modify the model\nThere are a couple of functions from the bayesplot package that interface with brms to evaluate your model. Most commonly, I will use the following functions to assess my model:\n\nPosterior predictive checking is one way to see how your model results compare with your observed data. You can run a posterior predictive check with the pp_check and specify several different types of checks. The most common ones I use are: “dens_overlay” and “intervals_grouped” if I am running a mixed model.\nTrace plot for your model using mcmc_plot to see if there was adequate chain mixing.\n\nAnother good test is leave-one-out cross-validation (LOOCV). LOOCV essentially checks the predictive ability of your model when part of the data is left out. This is helpful for understanding the influence of certain observations on your model fit. You can test on your model with loo.\n\n\n6. Compare models\nKeep all your model iterations. You can think of the Bayesian workflow as a process, where anything you do in this workflow is information about your data analysis.\nbrms aims to make it straightforward to adjust aspects of your model (e.g., model equation, priors, number of iterations) so that you can make comparisons between them. Once you have some models that you would like to compare, you can run loo_compare to see had the best predictive ability or run your other evaluation checks to compare fit."
  },
  {
    "objectID": "03_example.html",
    "href": "03_example.html",
    "title": "Example",
    "section": "",
    "text": "brms example\nWe are going to run through an example using orange dataset from R. For this example, I am going to load tidyverse and brms.\nWe also need to load in the actual dataset, which in this case is built into R.\nNow we can run through the Bayesian workflow process with brms to analyze this data. The two questions I have are 1) is age correlated with tree circumference and 2) does this differ depending on the tree?\nLet’s walk through the steps."
  },
  {
    "objectID": "03_example.html#pick-an-initial-model",
    "href": "03_example.html#pick-an-initial-model",
    "title": "Example",
    "section": "Pick an initial model",
    "text": "Pick an initial model\n\n## Explore shape of our data \nggplot(Orange, aes(circumference)) + \n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\nggplot(Orange, aes(age, circumference)) + \n  geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n## Pick a model \nset.seed(1992)\nm1 <- brm(circumference ~  age, data = Orange) \n\nCompiling Stan program...\nTrying to compile a simple C file\n\n\nRunning /Library/Frameworks/R.framework/Resources/bin/R CMD SHLIB foo.c\nclang -arch arm64 -I\"/Library/Frameworks/R.framework/Resources/include\" -DNDEBUG   -I\"/Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/library/Rcpp/include/\"  -I\"/Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/library/RcppEigen/include/\"  -I\"/Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/library/RcppEigen/include/unsupported\"  -I\"/Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/library/BH/include\" -I\"/Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/library/StanHeaders/include/src/\"  -I\"/Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/library/StanHeaders/include/\"  -I\"/Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/library/RcppParallel/include/\"  -I\"/Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/library/rstan/include\" -DEIGEN_NO_DEBUG  -DBOOST_DISABLE_ASSERTS  -DBOOST_PENDING_INTEGER_LOG2_HPP  -DSTAN_THREADS  -DBOOST_NO_AUTO_PTR  -include '/Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/library/StanHeaders/include/stan/math/prim/mat/fun/Eigen.hpp'  -D_REENTRANT -DRCPP_PARALLEL_USE_TBB=1   -I/opt/R/arm64/include   -fPIC  -falign-functions=64 -Wall -g -O2  -c foo.c -o foo.o\nIn file included from <built-in>:1:\nIn file included from /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/library/StanHeaders/include/stan/math/prim/mat/fun/Eigen.hpp:13:\nIn file included from /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/library/RcppEigen/include/Eigen/Dense:1:\nIn file included from /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/library/RcppEigen/include/Eigen/Core:88:\n/Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/library/RcppEigen/include/Eigen/src/Core/util/Macros.h:628:1: error: unknown type name 'namespace'\nnamespace Eigen {\n^\n/Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/library/RcppEigen/include/Eigen/src/Core/util/Macros.h:628:16: error: expected ';' after top level declarator\nnamespace Eigen {\n               ^\n               ;\nIn file included from <built-in>:1:\nIn file included from /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/library/StanHeaders/include/stan/math/prim/mat/fun/Eigen.hpp:13:\nIn file included from /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/library/RcppEigen/include/Eigen/Dense:1:\n/Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/library/RcppEigen/include/Eigen/Core:96:10: fatal error: 'complex' file not found\n#include <complex>\n         ^~~~~~~~~\n3 errors generated.\nmake: *** [foo.o] Error 1\n\n\nStart sampling\n\n\n\nSAMPLING FOR MODEL '1d353d396788ad4c5e23a98fea8e4db9' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 1.3e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.13 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.062169 seconds (Warm-up)\nChain 1:                0.017954 seconds (Sampling)\nChain 1:                0.080123 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL '1d353d396788ad4c5e23a98fea8e4db9' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 5e-06 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.05 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.029739 seconds (Warm-up)\nChain 2:                0.017502 seconds (Sampling)\nChain 2:                0.047241 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL '1d353d396788ad4c5e23a98fea8e4db9' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 7e-06 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.07 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.030291 seconds (Warm-up)\nChain 3:                0.016769 seconds (Sampling)\nChain 3:                0.04706 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL '1d353d396788ad4c5e23a98fea8e4db9' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 3e-06 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.03 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.050963 seconds (Warm-up)\nChain 4:                0.017686 seconds (Sampling)\nChain 4:                0.068649 seconds (Total)\nChain 4:"
  },
  {
    "objectID": "03_example.html#validate-computation",
    "href": "03_example.html#validate-computation",
    "title": "Example",
    "section": "Validate computation",
    "text": "Validate computation\n\nsummary(m1)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: circumference ~ age \n   Data: Orange (Number of observations: 35) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nPopulation-Level Effects: \n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept    17.25      9.05    -0.83    34.91 1.00     3726     2636\nage           0.11      0.01     0.09     0.12 1.00     3783     2712\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma    24.65      3.13    19.51    31.58 1.00     3110     2624\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nThe model results look good! Rhat = 1 and there is a pretty high effective sample size for the intercept and predictor. Therefore, we do not have any computational issues to address. Our model results suggest that age has a positive and significant effect on tree circumference (the 95% Bayesian credible interval is between 0.09 and 0.12). This can be interpreted as the circumference of the tree increases by 0.11 units for each one-unit increase in age.\nThe intercept shows that the estimated circumference of the tree at age 0 is 17.62, but there is a wide credible interval (0.55 to 35.12), there is quite a bit of uncertaintity with this estimate.\nSigma (family-specific parameter) tells us the variability in the response that is not explained by the model. In this case sigma is estimated to be 24.61 (3.12)."
  },
  {
    "objectID": "03_example.html#modify-model",
    "href": "03_example.html#modify-model",
    "title": "Example",
    "section": "Modify model",
    "text": "Modify model\nLet’s now make our model a little more complicated. The second question we are interested in exploring is tree-level differences.\n\nset.seed(1992)\nm2 <- brm(circumference ~ age + (1|Tree), \n                          data = Orange,\n                          warmup = 1000, #burn in period\n                          iter = 2000, # actual samples\n                          chains = 4,\n                          prior = c(prior(normal(0,1), class = b), # specify your mean and variance, weakly informative prior\n                                    prior(normal(0,1), class = Intercept)))\n\nCompiling Stan program...\n\n\nTrying to compile a simple C file\n\n\nRunning /Library/Frameworks/R.framework/Resources/bin/R CMD SHLIB foo.c\nclang -arch arm64 -I\"/Library/Frameworks/R.framework/Resources/include\" -DNDEBUG   -I\"/Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/library/Rcpp/include/\"  -I\"/Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/library/RcppEigen/include/\"  -I\"/Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/library/RcppEigen/include/unsupported\"  -I\"/Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/library/BH/include\" -I\"/Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/library/StanHeaders/include/src/\"  -I\"/Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/library/StanHeaders/include/\"  -I\"/Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/library/RcppParallel/include/\"  -I\"/Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/library/rstan/include\" -DEIGEN_NO_DEBUG  -DBOOST_DISABLE_ASSERTS  -DBOOST_PENDING_INTEGER_LOG2_HPP  -DSTAN_THREADS  -DBOOST_NO_AUTO_PTR  -include '/Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/library/StanHeaders/include/stan/math/prim/mat/fun/Eigen.hpp'  -D_REENTRANT -DRCPP_PARALLEL_USE_TBB=1   -I/opt/R/arm64/include   -fPIC  -falign-functions=64 -Wall -g -O2  -c foo.c -o foo.o\nIn file included from <built-in>:1:\nIn file included from /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/library/StanHeaders/include/stan/math/prim/mat/fun/Eigen.hpp:13:\nIn file included from /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/library/RcppEigen/include/Eigen/Dense:1:\nIn file included from /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/library/RcppEigen/include/Eigen/Core:88:\n/Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/library/RcppEigen/include/Eigen/src/Core/util/Macros.h:628:1: error: unknown type name 'namespace'\nnamespace Eigen {\n^\n/Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/library/RcppEigen/include/Eigen/src/Core/util/Macros.h:628:16: error: expected ';' after top level declarator\nnamespace Eigen {\n               ^\n               ;\nIn file included from <built-in>:1:\nIn file included from /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/library/StanHeaders/include/stan/math/prim/mat/fun/Eigen.hpp:13:\nIn file included from /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/library/RcppEigen/include/Eigen/Dense:1:\n/Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/library/RcppEigen/include/Eigen/Core:96:10: fatal error: 'complex' file not found\n#include <complex>\n         ^~~~~~~~~\n3 errors generated.\nmake: *** [foo.o] Error 1\n\n\nStart sampling\n\n\n\nSAMPLING FOR MODEL 'b0a4130d6b4fb0fc64f91696a1a1e78c' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 3.6e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.36 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.308471 seconds (Warm-up)\nChain 1:                0.329599 seconds (Sampling)\nChain 1:                0.63807 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'b0a4130d6b4fb0fc64f91696a1a1e78c' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 1.6e-05 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.16 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.366988 seconds (Warm-up)\nChain 2:                0.312643 seconds (Sampling)\nChain 2:                0.679631 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'b0a4130d6b4fb0fc64f91696a1a1e78c' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 8e-06 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.08 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.327698 seconds (Warm-up)\nChain 3:                0.341079 seconds (Sampling)\nChain 3:                0.668777 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'b0a4130d6b4fb0fc64f91696a1a1e78c' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 1.6e-05 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.16 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.357236 seconds (Warm-up)\nChain 4:                0.34725 seconds (Sampling)\nChain 4:                0.704486 seconds (Total)\nChain 4: \n\n## Assess model \nsummary(m2)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: circumference ~ age + (1 | Tree) \n   Data: Orange (Number of observations: 35) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nGroup-Level Effects: \n~Tree (Number of levels: 5) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)   123.75     40.60    70.03   225.36 1.01      527      871\n\nPopulation-Level Effects: \n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept   -98.45      5.28  -109.18   -88.37 1.00     2996     2180\nage           0.11      0.01     0.10     0.12 1.00     3050     2436\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma    15.95      2.32    12.35    21.55 1.00     2102     2112\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\nloo(m2) \n\n\nComputed from 4000 by 35 log-likelihood matrix\n\n         Estimate  SE\nelpd_loo   -149.7 3.5\np_loo         6.5 1.2\nlooic       299.4 7.1\n------\nMonte Carlo SE of elpd_loo is 0.1.\n\nPareto k diagnostic values:\n                         Count Pct.    Min. n_eff\n(-Inf, 0.5]   (good)     34    97.1%   885       \n (0.5, 0.7]   (ok)        1     2.9%   792       \n   (0.7, 1]   (bad)       0     0.0%   <NA>      \n   (1, Inf)   (very bad)  0     0.0%   <NA>      \n\nAll Pareto k estimates are ok (k < 0.7).\nSee help('pareto-k-diagnostic') for details.\n\n## Posterior predictive check: https://www.monicaalexander.com/posts/2020-28-02-bayes_viz/\npp_check(m2, type = \"dens_overlay\", nsamples = 100)\n\nWarning: Argument 'nsamples' is deprecated. Please use argument 'ndraws'\ninstead.\n\n\n\n\npp_check(m2, type = \"stat\", stat = 'median', nsamples = 100) \n\nWarning: Argument 'nsamples' is deprecated. Please use argument 'ndraws'\ninstead.\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\npp_check(m2,type = \"intervals_grouped\", group = \"Tree\") #grouped by Tree\n\nUsing all posterior draws for ppc type 'intervals_grouped' by default.\n\n\n\n\n## Error\nplot(m2) #shape should reflect the distribution of the model\n\n\n\n## Conditional effects\nconditional_effects(m2) #no group level effects without re_formula = NULL \n\n\n\n\nAgain, model convergence and sampling efficiency look good. The estimate & error look pretty similar to m1, however the intercept is very different. We can see that the standard deviation of the Tree intercepts is quite large, 124.49, with an error of 37.55. This suggests there is quite a bit of variation between trees."
  },
  {
    "objectID": "03_example.html#compare-models",
    "href": "03_example.html#compare-models",
    "title": "Example",
    "section": "Compare models",
    "text": "Compare models\nLastly, let’s compare our models\n\nloo_compare(loo(m1), loo(m2))\n\n   elpd_diff se_diff\nm2   0.0       0.0  \nm1 -12.7       4.0"
  },
  {
    "objectID": "04_resources.html#resources",
    "href": "04_resources.html#resources",
    "title": "Resources",
    "section": "Resources",
    "text": "Resources\nBürkner, Paul-Christian. (2017). brms: An R Package for Bayesian Multilevel Models Using Stan. Journal of Statistical Software, 80(1), 1–28. https://doi.org/10.18637/jss.v080.i01\nBürkner, Paul-Christian (2023). brms. https://paul-buerkner.github.io/brms/\nGelman, Andrew, Vehtari, Aki, Simpson, Daniel, Margossian, Charles C., Carpenter, Bobb, Yao, Yuling, Kennedy, Lauren, Gabry, Johah, Bürkner, Paul-Christian, and Modrak, Mark (2020). Bayesian Workflow. arXiv. https://doi.org/10.48550/arXiv.2011.01808\nKurz, A Solomon (2019). Statistical Rethinking with brms, ggplot2, and the tidyverse. https://bookdown.org/ajkurz/Statistical_Rethinking_recoded/\nMcElreath, R. (2016). Statistical rethinking: A Bayesian course with examples in R and Stan. Chapman & Hall/CRC Press. https://xcelab.net/rm/statistical-rethinking/"
  }
]